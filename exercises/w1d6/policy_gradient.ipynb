{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yV0d4QHYbY5W"
      },
      "source": [
        "## Vanilla Policy Optimisation\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d5/vanilla_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "Preliminary questions:\n",
        "- Run the script with the defaults parameters on the terminal\n",
        "- Explain from torch.distributions.categorical import Categorical\n",
        "- google gym python, why is it useful?\n",
        "- Policy gradient is model based or model free?\n",
        "- Is policy gradient on-policy or off-policy?\n",
        "\n",
        "Read all the code, then:\n",
        "- Complete the ... in the compute_loss function.\n",
        "- Use https://github.com/patrick-kidger/torchtyping to type the functions get_policy, get_action. You can draw inspiration from the compute_loss function.\n",
        "- Answer the questions\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSFiuc_pk80"
      },
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Policy gradient : if the sign of the reward is positive, then i want to take the policy more often.\n",
        "if the sign is negative, i will optimize in the opposite way"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6PDBFpFW_s7Y"
      },
      "source": [
        "## Batch size \n",
        "\n",
        "https://ai.stackexchange.com/questions/11640/how-large-should-the-replay-buffer-be\n",
        "\n",
        "Buffer lenght for experience replay \n",
        "\n",
        "The longer the more stable but slower "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5VrZBEG_r11",
        "outputId": "2b79bb23-81c3-4c27-e1e3-1eed4b81bf48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.12.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFBLe9pybY5d",
        "outputId": "56b1eee7-28dd-49cd-c30b-af391120a6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtyping\n",
            "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
            "Collecting typeguard>=2.11.1\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from torchtyping) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->torchtyping) (4.4.0)\n",
            "Installing collected packages: typeguard, torchtyping\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "Successfully installed torchtyping-0.1.4 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtyping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZHiD0b5bY5f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "from torchtyping import TensorType, patch_typeguard\n",
        "from typeguard import typechecked\n",
        "\n",
        "patch_typeguard()  # use before @typechecked\n",
        "\n",
        "\n",
        "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
        "    # Build a feedforward neural network.\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    # What does * mean here? Search for unpacking in python\n",
        "    # unpack list elements\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
        "          epochs=50, batch_size=5000, render=False):\n",
        "\n",
        "    # make environment, check spaces, get obs / act dims\n",
        "    env = gym.make(env_name)\n",
        "    assert isinstance(env.observation_space, Box), \\\n",
        "        \"This example only works for envs with continuous state spaces.\"\n",
        "    assert isinstance(env.action_space, Discrete), \\\n",
        "        \"This example only works for envs with discrete action spaces.\"\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_acts = env.action_space.n\n",
        "\n",
        "    # Core of policy network\n",
        "    # What should be the sizes of the layers of the policy network?\n",
        "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
        "\n",
        "    # make function to compute action distribution\n",
        "    # construct log-probabilities and probabilities for actions\n",
        "    # Categorical : a method for sampling from the distribution and for computing log probabilities of given samples\n",
        "    # What is the shape of obs?\n",
        "    @typechecked # To be typed\n",
        "    def get_policy(obs: TensorType[..., obs_dim]):\n",
        "        # Warning: obs has not always the same shape.\n",
        "        logits = logits_net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    # make action selection function (outputs int actions, sampled from policy)\n",
        "    # samples actions based on probabilities computed from the logits.\n",
        "    # What is the shape of obs?\n",
        "    @typechecked # To be typed\n",
        "    def get_action(obs: TensorType[obs_dim]) -> float:\n",
        "        return get_policy(obs).sample().item()\n",
        "\n",
        "    # make loss function whose gradient, for the right data, is policy gradient\n",
        "    # Right data : a set of (state, action, weight) tuples collected while acting according to the current policy\n",
        "    # where the weight for a state-action pair is the return from the episode to which it belongs\n",
        "    # log prob : get the logarithmic probability (logprob) of one experiment sample (sample) under a specific distribution (dist).\n",
        "    # What is the shape of obs?\n",
        "    @typechecked\n",
        "    def compute_loss(obs: TensorType[\"b\", obs_dim], act: TensorType[\"b\"], weights: TensorType[\"b\"]):\n",
        "        \"\"\"TODO\"\"\"\n",
        "        # https://stackoverflow.com/questions/54635355/what-does-log-prob-do\n",
        "        # Help: weights is a vector piecewise constant containing the total reward of each episode.\n",
        "        logprobs = get_policy(obs).log_prob(act)\n",
        "        return -(logprobs * weights).mean()\n",
        "\n",
        "\n",
        "    # make optimizer\n",
        "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
        "\n",
        "    # for training policy\n",
        "    def train_one_epoch():\n",
        "        # make some empty lists for logging.\n",
        "        batch_obs = []          # for observations\n",
        "        batch_acts = []         # for actions\n",
        "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
        "        batch_rets = []         # for measuring episode returns # What is the return?\n",
        "        batch_lens = []         # for measuring episode lengths\n",
        "\n",
        "        # reset episode-specific variables\n",
        "        obs = env.reset()       # first obs comes from starting distribution \n",
        "        done = False            # signal from environment that episode is over\n",
        "        ep_rews = []            # list for rewards accrued throughout ep\n",
        "\n",
        "        # render first episode of each epoch\n",
        "        finished_rendering_this_epoch = False\n",
        "\n",
        "        # collect experience by acting in the environment with current policy\n",
        "        while True:\n",
        "\n",
        "            # rendering\n",
        "            if (not finished_rendering_this_epoch) and render:\n",
        "                env.render()\n",
        "\n",
        "            # save obs\n",
        "            batch_obs.append(obs.copy())\n",
        "\n",
        "            # act in the environment\n",
        "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
        "            obs, rew, done, _ = env.step(act)\n",
        "\n",
        "            # save action, reward\n",
        "            batch_acts.append(act)\n",
        "            ep_rews.append(rew)\n",
        "\n",
        "            if done:\n",
        "                # if episode is over, record info about episode\n",
        "                # Is the reward discounted?\n",
        "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
        "                batch_rets.append(ep_ret)\n",
        "                batch_lens.append(ep_len)\n",
        "\n",
        "                # the weight for each logprob(a|s) is R(tau)\n",
        "                # Why do we use a constant vector here?\n",
        "                batch_weights += [ep_ret] * ep_len\n",
        "\n",
        "                # reset episode-specific variables\n",
        "                obs, done, ep_rews = env.reset(), False, []\n",
        "\n",
        "                # won't render again this epoch\n",
        "                finished_rendering_this_epoch = True\n",
        "\n",
        "                # end experience loop if we have enough of it\n",
        "                if len(batch_obs) > batch_size:\n",
        "                    break\n",
        "\n",
        "        # take a single policy gradient update step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
        "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
        "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
        "                                  )\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        return batch_loss, batch_rets, batch_lens\n",
        "\n",
        "    # training loop\n",
        "    for i in range(epochs):\n",
        "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
        "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
        "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQwv4uGNbY5h",
        "outputId": "5ec1966b-c8bd-40fa-9e30-b3abbca87a30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\emili\\anaconda3\\envs\\ML4G\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "C:\\Users\\emili\\AppData\\Local\\Temp\\ipykernel_22392\\2992695900.py:134: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
            "  batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:   0 \t loss: 16.802 \t return: 22.000 \t ep_len: 22.000\n",
            "epoch:   1 \t loss: 16.151 \t return: 18.333 \t ep_len: 18.333\n",
            "epoch:   2 \t loss: 14.199 \t return: 18.833 \t ep_len: 18.833\n",
            "epoch:   3 \t loss: 15.682 \t return: 22.000 \t ep_len: 22.000\n",
            "epoch:   4 \t loss: 19.384 \t return: 26.000 \t ep_len: 26.000\n",
            "epoch:   5 \t loss: 26.092 \t return: 38.000 \t ep_len: 38.000\n",
            "epoch:   6 \t loss: 25.207 \t return: 31.800 \t ep_len: 31.800\n",
            "epoch:   7 \t loss: 30.435 \t return: 37.667 \t ep_len: 37.667\n",
            "epoch:   8 \t loss: 32.058 \t return: 45.333 \t ep_len: 45.333\n",
            "epoch:   9 \t loss: 20.500 \t return: 29.000 \t ep_len: 29.000\n",
            "epoch:  10 \t loss: 22.755 \t return: 33.667 \t ep_len: 33.667\n",
            "epoch:  11 \t loss: 26.941 \t return: 41.667 \t ep_len: 41.667\n",
            "epoch:  12 \t loss: 26.791 \t return: 42.000 \t ep_len: 42.000\n",
            "epoch:  13 \t loss: 23.815 \t return: 38.667 \t ep_len: 38.667\n",
            "epoch:  14 \t loss: 34.324 \t return: 49.667 \t ep_len: 49.667\n",
            "epoch:  15 \t loss: 31.542 \t return: 46.333 \t ep_len: 46.333\n",
            "epoch:  16 \t loss: 36.429 \t return: 38.333 \t ep_len: 38.333\n",
            "epoch:  17 \t loss: 30.344 \t return: 39.750 \t ep_len: 39.750\n",
            "epoch:  18 \t loss: 23.136 \t return: 37.667 \t ep_len: 37.667\n",
            "epoch:  19 \t loss: 42.874 \t return: 58.000 \t ep_len: 58.000\n",
            "epoch:  20 \t loss: 20.987 \t return: 32.750 \t ep_len: 32.750\n",
            "epoch:  21 \t loss: 41.198 \t return: 49.000 \t ep_len: 49.000\n",
            "epoch:  22 \t loss: 23.638 \t return: 37.333 \t ep_len: 37.333\n",
            "epoch:  23 \t loss: 34.572 \t return: 54.500 \t ep_len: 54.500\n",
            "epoch:  24 \t loss: 24.933 \t return: 37.333 \t ep_len: 37.333\n",
            "epoch:  25 \t loss: 20.506 \t return: 37.667 \t ep_len: 37.667\n",
            "epoch:  26 \t loss: 23.704 \t return: 37.000 \t ep_len: 37.000\n",
            "epoch:  27 \t loss: 21.687 \t return: 35.500 \t ep_len: 35.500\n",
            "epoch:  28 \t loss: 34.342 \t return: 50.667 \t ep_len: 50.667\n",
            "epoch:  29 \t loss: 25.398 \t return: 40.667 \t ep_len: 40.667\n",
            "epoch:  30 \t loss: 34.616 \t return: 53.000 \t ep_len: 53.000\n",
            "epoch:  31 \t loss: 33.296 \t return: 57.000 \t ep_len: 57.000\n",
            "epoch:  32 \t loss: 42.768 \t return: 64.000 \t ep_len: 64.000\n",
            "epoch:  33 \t loss: 28.807 \t return: 53.500 \t ep_len: 53.500\n",
            "epoch:  34 \t loss: 34.321 \t return: 52.000 \t ep_len: 52.000\n",
            "epoch:  35 \t loss: 29.315 \t return: 50.500 \t ep_len: 50.500\n",
            "epoch:  36 \t loss: 50.713 \t return: 60.000 \t ep_len: 60.000\n",
            "epoch:  37 \t loss: 46.670 \t return: 83.000 \t ep_len: 83.000\n",
            "epoch:  38 \t loss: 43.925 \t return: 73.000 \t ep_len: 73.000\n",
            "epoch:  39 \t loss: 31.292 \t return: 54.000 \t ep_len: 54.000\n",
            "epoch:  40 \t loss: 22.909 \t return: 42.667 \t ep_len: 42.667\n",
            "epoch:  41 \t loss: 28.717 \t return: 53.500 \t ep_len: 53.500\n",
            "epoch:  42 \t loss: 25.709 \t return: 44.000 \t ep_len: 44.000\n",
            "epoch:  43 \t loss: 30.507 \t return: 50.667 \t ep_len: 50.667\n",
            "epoch:  44 \t loss: 58.006 \t return: 101.000 \t ep_len: 101.000\n",
            "epoch:  45 \t loss: 34.315 \t return: 59.500 \t ep_len: 59.500\n",
            "epoch:  46 \t loss: 29.056 \t return: 53.500 \t ep_len: 53.500\n",
            "epoch:  47 \t loss: 24.309 \t return: 48.000 \t ep_len: 48.000\n",
            "epoch:  48 \t loss: 31.220 \t return: 61.500 \t ep_len: 61.500\n",
            "epoch:  49 \t loss: 25.572 \t return: 44.667 \t ep_len: 44.667\n",
            "epoch:  50 \t loss: 41.169 \t return: 71.500 \t ep_len: 71.500\n",
            "epoch:  51 \t loss: 44.175 \t return: 85.500 \t ep_len: 85.500\n",
            "epoch:  52 \t loss: 42.318 \t return: 74.000 \t ep_len: 74.000\n",
            "epoch:  53 \t loss: 25.206 \t return: 43.000 \t ep_len: 43.000\n",
            "epoch:  54 \t loss: 35.893 \t return: 55.667 \t ep_len: 55.667\n",
            "epoch:  55 \t loss: 34.151 \t return: 56.500 \t ep_len: 56.500\n",
            "epoch:  56 \t loss: 29.523 \t return: 51.333 \t ep_len: 51.333\n",
            "epoch:  57 \t loss: 45.080 \t return: 84.500 \t ep_len: 84.500\n",
            "epoch:  58 \t loss: 37.345 \t return: 75.000 \t ep_len: 75.000\n",
            "epoch:  59 \t loss: 75.768 \t return: 145.000 \t ep_len: 145.000\n",
            "epoch:  60 \t loss: 58.690 \t return: 126.000 \t ep_len: 126.000\n",
            "epoch:  61 \t loss: 27.299 \t return: 54.000 \t ep_len: 54.000\n",
            "epoch:  62 \t loss: 62.774 \t return: 85.667 \t ep_len: 85.667\n",
            "epoch:  63 \t loss: 38.741 \t return: 61.500 \t ep_len: 61.500\n",
            "epoch:  64 \t loss: 27.516 \t return: 54.000 \t ep_len: 54.000\n",
            "epoch:  65 \t loss: 57.905 \t return: 124.000 \t ep_len: 124.000\n",
            "epoch:  66 \t loss: 28.703 \t return: 56.333 \t ep_len: 56.333\n",
            "epoch:  67 \t loss: 35.836 \t return: 73.000 \t ep_len: 73.000\n",
            "epoch:  68 \t loss: 18.390 \t return: 38.667 \t ep_len: 38.667\n",
            "epoch:  69 \t loss: 26.637 \t return: 46.000 \t ep_len: 46.000\n",
            "epoch:  70 \t loss: 27.500 \t return: 51.500 \t ep_len: 51.500\n",
            "epoch:  71 \t loss: 59.081 \t return: 122.000 \t ep_len: 122.000\n",
            "epoch:  72 \t loss: 19.585 \t return: 40.667 \t ep_len: 40.667\n",
            "epoch:  73 \t loss: 19.606 \t return: 41.000 \t ep_len: 41.000\n",
            "epoch:  74 \t loss: 29.315 \t return: 61.000 \t ep_len: 61.000\n",
            "epoch:  75 \t loss: 19.243 \t return: 45.667 \t ep_len: 45.667\n",
            "epoch:  76 \t loss: 17.177 \t return: 35.667 \t ep_len: 35.667\n",
            "epoch:  77 \t loss: 19.934 \t return: 41.000 \t ep_len: 41.000\n",
            "epoch:  78 \t loss: 20.852 \t return: 44.667 \t ep_len: 44.667\n",
            "epoch:  79 \t loss: 26.841 \t return: 52.500 \t ep_len: 52.500\n",
            "epoch:  80 \t loss: 23.943 \t return: 45.667 \t ep_len: 45.667\n",
            "epoch:  81 \t loss: 21.309 \t return: 41.333 \t ep_len: 41.333\n",
            "epoch:  82 \t loss: 19.157 \t return: 39.667 \t ep_len: 39.667\n",
            "epoch:  83 \t loss: 16.799 \t return: 39.000 \t ep_len: 39.000\n",
            "epoch:  84 \t loss: 21.483 \t return: 46.667 \t ep_len: 46.667\n",
            "epoch:  85 \t loss: 17.065 \t return: 34.250 \t ep_len: 34.250\n",
            "epoch:  86 \t loss: 23.990 \t return: 39.667 \t ep_len: 39.667\n",
            "epoch:  87 \t loss: 21.717 \t return: 44.333 \t ep_len: 44.333\n",
            "epoch:  88 \t loss: 19.646 \t return: 41.000 \t ep_len: 41.000\n",
            "epoch:  89 \t loss: 18.877 \t return: 38.000 \t ep_len: 38.000\n",
            "epoch:  90 \t loss: 22.485 \t return: 42.333 \t ep_len: 42.333\n",
            "epoch:  91 \t loss: 20.546 \t return: 39.333 \t ep_len: 39.333\n",
            "epoch:  92 \t loss: 15.989 \t return: 35.000 \t ep_len: 35.000\n",
            "epoch:  93 \t loss: 14.983 \t return: 29.250 \t ep_len: 29.250\n",
            "epoch:  94 \t loss: 12.890 \t return: 26.250 \t ep_len: 26.250\n",
            "epoch:  95 \t loss: 17.116 \t return: 36.667 \t ep_len: 36.667\n",
            "epoch:  96 \t loss: 12.660 \t return: 28.000 \t ep_len: 28.000\n",
            "epoch:  97 \t loss: 15.322 \t return: 35.667 \t ep_len: 35.667\n",
            "epoch:  98 \t loss: 15.642 \t return: 35.000 \t ep_len: 35.000\n",
            "epoch:  99 \t loss: 16.579 \t return: 29.500 \t ep_len: 29.500\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "train(env_name='CartPole-v0', hidden_sizes=[48], lr=1e-2, \n",
        "          epochs=100, batch_size=100, render=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MGtVtfDbY5i"
      },
      "outputs": [],
      "source": [
        "# Original algo here: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/vpg/vpg.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML4G",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8c54bd2db42eef7c1afee12aea437de293788b38ec6a5c92d00aed8b4840e806"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
