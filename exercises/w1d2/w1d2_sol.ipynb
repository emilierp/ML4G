{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# W1D2 - as_strided, einsum, and Build Your Own ResNet\n",
    "\n",
    "ResNet is a architecture named after its residual connections. In the 2015 paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf), the authors demonstrated with a winning solution to ILVSRC 2015 that  residual connections were a solution to the problem of training neural networks with hundreds of layers.\n",
    "\n",
    "At this time, it was difficult to train networks with more than tens of layers. Even with normalization layers to prevent gradients from vanishing or exploding, and even with robust data augmentation to reduce overfitting, optimizers like SGD struggled to find solutions with low training error, let alone good generalization.\n",
    "\n",
    "The authors provided empirical evidence that ResNets were effective, and in recent years the theory of residual connections has developed. For example, we now know that [Residual Networks Behave Like Ensembles of Relatively Shallow Networks](https://arxiv.org/pdf/1605.06431.pdf), and that under some conditions ResNets are [guaranteed to converge to a global (not local!) optimum in polynomial time](https://arxiv.org/pdf/1811.03804.pdf).\n",
    "\n",
    "The ResNet we'll build today is no longer state of the art, but you'll find residual connections appearing again and again throughout the course, so it's worth understanding this idea in detail.\n",
    "\n",
    "<!-- toc -->\n",
    "\n",
    "## Readings\n",
    "\n",
    "- [Batch Normalization in Convolutional Neural Networks](https://www.baeldung.com/cs/batch-normalization-cnn)\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "## Inference Using a Pre-Trained ResNet\n",
    "\n",
    "First, we'll load a pre-trained ResNet34 from the [torchvision.models](https://pytorch.org/vision/stable/models.html) package and use it to classify some images. The 34 refers to the number of layers in the model.\n",
    "\n",
    "There are at least three things that are easy to forget. If you forget these things then your model will still run without any errors, and will likely even perform decently, which makes it tricky to even detect that you've done something wrong. Unfortunately, this is just the reality of working on ML code.\n",
    "\n",
    "### Gotcha 1 - Training vs Eval Mode\n",
    "\n",
    "By default, the model is in training mode when loaded, but we want it to be in evaluation mode for inference. This means that `model.training` is True, and for each submodule their corresponding flag like `model.bn1.training` is True as well. Calling `model.eval()` sets the flag to False on the model and recursively on all submodels.\n",
    "\n",
    "What does this actually do? The two cases you'll need to know for the course are batch normalization and dropout layers.\n",
    "\n",
    "- Batch normalization in training mode calculates mean and variance based on the current batch, which can be ok-ish or really bad depending on the size of your batches at inference time. In eval mode it uses aggregated statistics saved in the earlier training process.\n",
    "- Dropout in training mode randomly sets activations to zero and rescales the others. In eval mode it does nothing.\n",
    "\n",
    "### Gotcha 2 - Input Normalization\n",
    "\n",
    "The ResNet was trained on images that were normalized using special constants, which are in the PyTorch documentation. It thus expects any images you give it to be also normalized using these special constants. If you forget to do this, you might see the model still gives the right classification, but the logits are small for every class because the image doesn't quite look like anything it was trained on.\n",
    "\n",
    "Another way I have personally messed this up is if by using images that are already normalized, but you normalize them a second time by mistake.\n",
    "\n",
    "### Gotcha 3 - no_grad / inference_mode\n",
    "\n",
    "PyTorch is built around making backpropagation easy, and by default it does some extra work just in case you want to run backpropagation later. This is especially true on tensors that have `requires_grad` set to True, which pretrained model does by default. When you implement your own backpropagation later, you'll understand this in more detail.\n",
    "\n",
    "We can tell PyTorch not to do any of this extra work by wrapping calls to our model in `with t.inference_mode():` as described in the [PyTorch docs](https://pytorch.org/docs/stable/autograd.html#locally-disabling-gradient-computation).\n",
    "\n",
    "If you've seen `with t.no_grad():` before, `inference_mode` is a newer feature (released in PyTorch 1.9) that does everything `no_grad` does plus a couple more optimizations.\n",
    "\n",
    "### The `tqdm` library\n",
    "\n",
    "The [tqdm](https://pypi.org/project/tqdm/) library provides nice progress bars so you can see how long operations are going to take. It's very easy to use - just wrap your iterable in `tqdm()`. I recommend using `tqdm` for everything in the course that takes more than a couple seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mt\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39meinops\u001b[39;00m \u001b[39mimport\u001b[39;00m rearrange\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m display\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\causal\\Lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodulefinder\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\causal\\Lib\\site-packages\\torchvision\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optical_flow\u001b[39;00m \u001b[39mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stereo_matching\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     CarlaStereo,\n\u001b[0;32m      4\u001b[0m     CREStereo,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     SintelStereo,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcaltech\u001b[39;00m \u001b[39mimport\u001b[39;00m Caltech101, Caltech256\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\causal\\Lib\\site-packages\\torchvision\\datasets\\_optical_flow.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_png_16\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_pfm, verify_str_arg\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\causal\\Lib\\site-packages\\PIL\\Image.py:103\u001b[0m\n\u001b[0;32m     94\u001b[0m MAX_IMAGE_PIXELS \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _imaging \u001b[39mas\u001b[39;00m core\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m __version__ \u001b[39m!=\u001b[39m \u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    106\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    107\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCore version: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mgetattr\u001b[39m(core, \u001b[39m'\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPillow version: \u001b[39m\u001b[39m{\u001b[39;00m__version__\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from io import BytesIO\n",
    "from typing import Callable, Optional, Union, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import torch as t\n",
    "import torchvision\n",
    "from einops import rearrange\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torch import nn\n",
    "from torch.nn.functional import conv1d as torch_conv1d\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import utils\n",
    "import w1d2_test\n",
    "\n",
    "if \"SKIP\":\n",
    "    DEBUG_SKIP_VISUALS = False\n",
    "MAIN = __name__ == \"__main__\"\n",
    "IS_CI = os.getenv(\"IS_CI\")\n",
    "images: List[Image.Image] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Caching Images\n",
    "\n",
    "We've provided some sample images; feel free to call `download_image` some URLs of your choice and append them to the `IMAGES` list. It's polite to cache the images locally instead of downloading them every time the notebook is run, to minimize bandwidth usage.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>I get UnidentifiedImageError when I try to load my own image!</summary>\n",
    "\n",
    "This usually means the image format isn't supported on your OS and version of the image library. You can try manually converting the image to a common format like JPEG, or just use another image.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_FOLDER = Path(\"./w1d2_images\")\n",
    "IMAGE_FOLDER.mkdir(exist_ok=True)\n",
    "IMAGE_FILENAMES = [\n",
    "    \"chimpanzee.jpg\",\n",
    "    \"golden_retriever.jpg\",\n",
    "    \"platypus.jpg\",\n",
    "    \"frogs.jpg\",\n",
    "    \"fireworks.jpg\",\n",
    "    \"astronaut.jpg\",\n",
    "    \"iguana.jpg\",\n",
    "    \"volcano.jpg\",\n",
    "    \"goofy.jpg\",\n",
    "    \"dragonfly.jpg\",\n",
    "]\n",
    "\n",
    "\n",
    "def download_image(url: str, filename: Optional[str]) -> None:\n",
    "    \"\"\"Download the image at url to w1d2_images/{filename}, if not already cached.\"\"\"\n",
    "    if filename is None:\n",
    "        filename = url.rsplit(\"/\", 1)[1].replace(\"%20\", \"\")\n",
    "    path = IMAGE_FOLDER / filename\n",
    "    if not path.exists():\n",
    "        response = requests.get(url)\n",
    "        data = response.content\n",
    "        with path.open(\"wb\") as f:\n",
    "            f.write(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MAIN:\n",
    "    images = [Image.open(IMAGE_FOLDER / filename) for filename in tqdm(IMAGE_FILENAMES)]\n",
    "    if not IS_CI:\n",
    "        display(images[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Torchvision Transforms\n",
    "\n",
    "Classes in the `transforms` module are either subclasses of `nn.Module`, or behave similarly in that they are callable. You can compose multiple transforms together with `transforms.Compose` or `nn.Sequential`.\n",
    "\n",
    "Using the classes in the `transforms` module, define a variable `preprocess` to be a composition of `transforms.ToTensor`, `transforms.Resize`, and `transforms.Normalize`.\n",
    "\n",
    "Resize every image to (224, 224) - while the model can support larger images than this, making them all of equal size is necessary for batching.\n",
    "\n",
    "Find the normalization constants in the [documentation](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "It's always a good idea to visualize your data after preprocessing to catch any errors, and get a feeling for what the model \"sees\". You can use [`plt.imshow`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) to display a tensor as an image, but you'll need to `rearrange` the data to the format it expects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess: Callable[[Image.Image], t.Tensor]\n",
    "\n",
    "if \"SOLUTION\":\n",
    "    if MAIN:\n",
    "        preprocess = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        processed: t.Tensor = preprocess(images[0])  # type: ignore\n",
    "        if not IS_CI:\n",
    "            plt.imshow(rearrange(processed, \"c h w -> h w c\"))\n",
    "else:\n",
    "    preprocess = transforms.Compose([\"your code here\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Image Preprocessing\n",
    "\n",
    "Implement `prepare_data` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(images: List[Image.Image]) -> t.Tensor:\n",
    "    \"\"\"Preprocess each image and stack them into a single tensor.\n",
    "\n",
    "    Return: shape (batch=len(images), num_channels=3, height=224, width=224)\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    x = t.stack([preprocess(img) for img in tqdm(images)], dim=0)  # type: ignore\n",
    "    assert isinstance(x, t.Tensor) and x.shape == (len(images), 3, 224, 224)\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ImageNet Class Names\n",
    "\n",
    "Use the following code to load the class labels for the 1000 classes in ImageNet.\n",
    "\n",
    "If you haven't encountered ImageNet before, spend a minute looking at the various categories to appreciate just how difficult this classification task is. When we train the model, we give it only 1 correct answer and all the others are considered equally incorrect. For example, given a photo of a dog, knowing that this is a dog or even a terrier is not enough: the model has to distinguish 29 different categories of terrier.\n",
    "\n",
    "Some of the labels are duplicates for no apparent reason: \"laptop computer\" is a different category than \"notebook computer\" and \"projectile, missile\" is different from \"missile\". Another issue with the problem setting is that when more than one label truly applies, the model has to infer which one the human labeller had in mind. For example, a scene with a typical desktop PC can contain \"desktop computer\", \"desk\", \"mouse\", \"monitor\", and \"computer keyboard\" categories but the labeller would have only specified one of these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"w1d2_imagenet_labels.json\") as f:\n",
    "    imagenet_labels = list(json.load(f).values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Making Predictions (pretrained model)\n",
    "\n",
    "Implement `predict` below and call it with your images. Remember the gotcha discussed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, images: List[Image.Image], print_topk_preds=3) -> List[int]:\n",
    "    \"\"\"\n",
    "    Pass the images through the model and print out the top predictions.\n",
    "\n",
    "    For each image, `display()` the image and the most likely categories according to the model.\n",
    "\n",
    "    Return: for each image, the index of the top prediction.\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    if DEBUG_SKIP_VISUALS:\n",
    "        return []\n",
    "\n",
    "    model.eval()\n",
    "    x = prepare_data(images)\n",
    "    with t.inference_mode():\n",
    "        out = model(x)\n",
    "\n",
    "    probs = out.softmax(dim=-1)\n",
    "    assert probs.shape == (len(images), len(imagenet_labels))\n",
    "    _, indices = out.topk(print_topk_preds, dim=-1)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        small = image.copy()\n",
    "        small.thumbnail((150, 150))\n",
    "        display(small)\n",
    "        print(\"\\n\".join(f\"{100*probs[i][j]:.4f}% {imagenet_labels[j]}\" for j in indices[i]))\n",
    "\n",
    "    return [ind[0].item() for ind in indices]\n",
    "\n",
    "\n",
    "if MAIN and not IS_CI:\n",
    "    model = models.resnet34(weights=\"DEFAULT\")\n",
    "    pretrained_categories = predict(model, images)\n",
    "    print(pretrained_categories)\n",
    "    if \"SKIP\":\n",
    "        # Sanity check for regressions - if student changed images these wouldn't match anymore\n",
    "        assert pretrained_categories == [\n",
    "            367,\n",
    "            207,\n",
    "            103,\n",
    "            865,\n",
    "            562,\n",
    "            628,\n",
    "            39,\n",
    "            980,\n",
    "            447,\n",
    "            954,\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Practice with `einsum` and `as_strided`\n",
    "\n",
    "Now instead of using torchvision's code for ResNet, we're going to implement our own ResNet! This requires being comfortable with `einsum` and `as_strided`.\n",
    "\n",
    "If you didn't do the pre-exercises on `as_strided`, it might be worth doing those now. Note that `x.as_strided(...)` is a method equivalent to `torch.as_strided(x, ...)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Trace of a matrix\n",
    "\n",
    "Implement the following functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def einsum_trace(a: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Compute the trace of the square 2D input using einsum.\"\"\"\n",
    "    \"SOLUTION\"\n",
    "    return t.einsum(\"ii->\", a)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_trace(einsum_trace)\n",
    "    w1d2_test.test_trace_transpose(einsum_trace)\n",
    "    w1d2_test.test_trace_expand(einsum_trace)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def as_strided_trace(a: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Compute the trace of the square 2D input using as_strided and sum.\n",
    "\n",
    "    Tip: the stride argument to `as_strided` must account for the stride of the inputs `a.stride()`.\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    N, _ = a.shape\n",
    "    stride = a.stride()\n",
    "    assert len(stride) == 2\n",
    "    a_strided = a.as_strided(size=(N,), stride=(stride[0] + stride[1],))\n",
    "    return t.sum(a_strided)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_trace(as_strided_trace)\n",
    "    w1d2_test.test_trace_transpose(as_strided_trace)\n",
    "    w1d2_test.test_trace_expand(as_strided_trace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "Implement the following functions.\n",
    "\n",
    "The implementation of `as_strided_matmul` should do the same thing as `einsum_matmul`, but using only as_strided, product, and sum operations. This is hard, but fun!\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hint 1 for as_strided_matmul: description of algorithm</summary>\n",
    "\n",
    "$$A = \\begin{bmatrix}r_1 \\\\ r_2 \\end{bmatrix} \\;\\;\\;\\;\n",
    "B = \\begin{bmatrix}c_1 && c_2 && c_3 \\end{bmatrix}$$\n",
    "\n",
    "Where $r_1$ and $r_2$ are row vectors and $c_1$, $c_2$, and $c_3$ are column vectors. All of these vectors should have the same length.\n",
    "\n",
    "$$A_{repeated} = \\begin{bmatrix}\n",
    "\tr_1 && r_1 && r_1 \\\\\n",
    "\tr_2 && r_2 && r_2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "$$B_{repeated} = \\begin{bmatrix}\n",
    "\tc_1 && c_2 && c_3 \\\\\n",
    "\tc_1 && c_2 && c_3 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$A_{repeated}$ and $B_{repeated}$ are both 3-tensors. You can imagine the $r$ and $c$ vectors as \"going into the page\" if that's helpful.\n",
    "\n",
    "These vectors are now lined up to do elementwise dot products to get the product of the original matrices.\n",
    "\n",
    "$$A \\times B = \\begin{bmatrix}\n",
    "\tr_1 \\cdot c_1 && r_1 \\cdot c_2 && r_1 \\cdot c_3 \\\\\n",
    "\tr_2 \\cdot c_1 && r_2 \\cdot c_2 && r_2 \\cdot c_3 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 for as_strided_matmul: using as_strided for repititions</summary>\n",
    "You can use a stride of 0 in as_strided to repeat along that dimension.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 for as_strided_matmul: Reading from non-contiguous tensors</summary>\n",
    "It's possible that the input matrices you recieve could themselves be the output of an as_strided operation, so that they're represented in memory in a non-contiguous way.\n",
    "Make sure that your as_strided operation is using the strides from the original input matrix on the dimensions that aren't being repeated.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def einsum_matmul(a: t.Tensor, b: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Matrix multiply 2D matrices a and b (same as a @ b).\"\"\"\n",
    "    \"SOLUTION\"\n",
    "    return t.einsum(\"ik,kj->ij\", a, b)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_matmul(einsum_matmul)\n",
    "    w1d2_test.test_matmul_transpose(einsum_matmul)\n",
    "    w1d2_test.test_matmul_expand(einsum_matmul)\n",
    "    w1d2_test.test_matmul_skip(einsum_matmul)\n",
    "\n",
    "# %%\n",
    "def as_strided_matmul(a: t.Tensor, b: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Matrix multiply 2D matrices a and b (same as a @ b), but use as_strided this time.\n",
    "\n",
    "    Use elementwise multiplication and sum.\n",
    "\n",
    "    Tip: the stride argument to `as_strided` must account for the stride of the inputs `a.stride()` and `b.stride()`.\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    A0, A1 = a.shape\n",
    "    B0, B1 = b.shape\n",
    "    assert A1 == B0\n",
    "\n",
    "    AS = a.stride()\n",
    "    assert len(AS) == 2\n",
    "    BS = b.stride()\n",
    "    assert len(BS) == 2\n",
    "    # Naive version of stride A1, 1, 0 is flawed\n",
    "    a_strided = a.as_strided(size=(A0, A1, B1), stride=(AS[0], AS[1], 0))\n",
    "    b_strided = b.as_strided(size=(A0, A1, B1), stride=(0, BS[0], BS[1]))\n",
    "    return (a_strided * b_strided).sum(dim=1)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_matmul(as_strided_matmul)\n",
    "    w1d2_test.test_matmul_transpose(as_strided_matmul)\n",
    "    w1d2_test.test_matmul_expand(as_strided_matmul)\n",
    "    w1d2_test.test_matmul_skip(as_strided_matmul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch's `conv1d` Exercises\n",
    "\n",
    "PyTorch's `conv1d` operates on 3D tensors, and it's easy to get confused on the shapes involved. To practice, for each test, `reshape` the variable `fix_me` so that the output of the `torch_conv1d` call matches the expected tensor.\n",
    "\n",
    "From the docs:\n",
    "- The \"input\" arg to conv1d should be shape (batch_size, in_channels, in_len)\n",
    "- The \"weight\" arg should be shape (out_channels, in_channels, kernel_width)\n",
    "- The output of this function will be shape (batch_size, out_channels, out_len), where out_len will be in_len - kernel_width + 1 (if stride is 1)\n",
    "\n",
    "In these problems, we'll give you two of these three tensors. You can infer the correct shape for the other one, and reshape it accordingly.\n",
    "\n",
    "Reminder: when there are multiple output channels, each output channel is computed independently as a function of _all_ the input channels.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Solution to A</summary>\n",
    "\n",
    "The input is (batch=1, in_channels=1, width=4).\n",
    "The output should be (batch=1, out_channels=1, out_width=3).\n",
    "\n",
    "The weights should be size (out_channels, in_channels, kernel_width), i.e. (1, 1, 2)\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Solution to B</summary>\n",
    "\n",
    "This is a convolution with a width-1 kernel that spans both input channels, producing 1 output channel\n",
    "\n",
    "The weights have (out_channels=1, in_channels=2, kernel_width=1)\n",
    "The output has (batch=2, out_channels=1, out_width=3)\n",
    "\n",
    "For the input shape, use `out_width = width - kernel_width + 1` to solve `width=3`. So it should be (2, 2, 3)\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Solution to C</summary>\n",
    "\n",
    "This is a convolution with two filters (i.e. two output channels), each of which operate independently on the single input channel.\n",
    "\n",
    "The weights have (out_channels=2, in_channels=1, kernel_width=1)\n",
    "The output has (batch=6, out_channels=2, out_width=1)\n",
    "\n",
    "For the input shape, use `out_width = width - kernel_width + 1` to solve `width=1`. So it should be shape (6, 1, 1)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MAIN:\n",
    "    input = t.tensor([[[1, 2, 3, 4]]])\n",
    "    fix_me = t.tensor([1, 3])\n",
    "    if \"SOLUTION\":\n",
    "        actual = torch_conv1d(input, fix_me.reshape((1, 1, 2)))\n",
    "    else:\n",
    "        actual = torch_conv1d(input, fix_me)\n",
    "    expected = t.tensor([7.0, 11.0, 15.0])\n",
    "    utils.test_is_equal(actual, expected, \"w1d2_test.conv1d_a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MAIN:\n",
    "    if \"SOLUTION\":\n",
    "        fix_me = t.arange(12).reshape((2, 2, 3))\n",
    "    else:\n",
    "        fix_me = t.arange(12)\n",
    "    weights = t.tensor([[[1], [-1]]])\n",
    "    actual = torch_conv1d(fix_me, weights)\n",
    "    expected = t.tensor([[[-3, -3, -3]], [[-3, -3, -3]]])\n",
    "    utils.test_is_equal(actual, expected, \"w1d2_test.conv1d_b\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MAIN:\n",
    "    if \"SOLUTION\":\n",
    "        fix_me = t.arange(6).view(6, 1, 1)\n",
    "    else:\n",
    "        fix_me = t.arange(6)\n",
    "\n",
    "    weights = t.tensor([[[-1]], [[1]]])\n",
    "    actual = torch_conv1d(fix_me, weights)\n",
    "    expected = t.tensor([[[0], [0]], [[-1], [1]], [[-2], [2]], [[-3], [3]], [[-4], [4]], [[-5], [5]]])\n",
    "    utils.test_is_equal(actual, expected, \"w1d2_test.conv1d_c\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Your Own `conv1d`\n",
    "\n",
    "Now implement your own minimal (i.e. assume padding = 0 and stride = 1) version of `conv1d`. It should use an `einsum` and an `as_strided` with careful consideration of the input strides.\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 - creating strided x</summary>\n",
    "You should use as_strided to make a strided version of x, which should have the shape (batch, input_channels, output_width, kernel_width).\n",
    "\n",
    "Like we did before, the output width can be calculated as `input_width - kernel_width + 1`.\n",
    "\n",
    "When you do as_strided, make sure to use appropriate stride values from x.stride().\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 - using strided x and the weights to get the final output</summary>\n",
    "You should use einsum on your strided version of x and the input weights to get your final output. You should contract (elementwise product and then sum) over the input channels and kernel width.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv1d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Like torch's conv1d using bias=False and all other keyword arguments left at their default values.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_width)\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    B, xC, xW = x.shape\n",
    "    _, kIC, kW = weights.shape\n",
    "    assert xC == kIC\n",
    "    xS = x.stride()  # typically, (xC*xW, xW, 1)\n",
    "    assert len(xS) == 3\n",
    "    conv_size = (B, xC, xW - kW + 1, kW)\n",
    "    conv_stride = (xS[0], xS[1], 1 * xS[2], xS[2])\n",
    "    strided_x = x.as_strided(size=conv_size, stride=conv_stride)\n",
    "    return t.einsum(\"bcnw,ocw->bon\", strided_x, weights)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_conv1d_minimal(conv1d_minimal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Your Own `conv2d`\n",
    "\n",
    "Now write your own 2D version. Reminder: the 2 in 2D means the weights slide over the input in two independent dimensions. The inputs and weights, and outputs will all be 4D.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hint 1 - shape of strided x</summary>\n",
    "\n",
    "The shape of `x` strided should be: (batch, in_channels, output_height, output_width, kernel_height, kernel_width)\n",
    "\n",
    "We can generalize our 1D output shape calculation to 2D as follows: `output_height = input_height - kernel_height + 1` and `output_width = input_width - kernel_width + 1`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hint 2 - strides of strided x</summary>\n",
    "\n",
    "The first four strides are the strides of the original `x`.\n",
    "For the kernel_height dimension, increasing by 1 means we want to increase by 1 in the height of `x`, so it should be `x.stride()[2]` and similarly for the kernel_width dimension.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 - final einsum</summary>\n",
    "The einsum should contract (elementwise product and then sum) over the input channels, kernel height, and kernel width dimensions.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Why does my version give slightly different output when using float32 compared to PyTorch's version?</summary>\n",
    "\n",
    "In floating point, addition is not associative: the sum of two `float32` is generally not representable exactly as a `float32`, so the sum has to be rounded. This means the order that the implementation performs reductions will affect the final result. For example, summing out a vector of four elements could be done from left to right like this:\n",
    "\n",
    "`((x[0] + x[1]) + x[2]) + x[3]`\n",
    "\n",
    "Or it could be done recursively like this:\n",
    "\n",
    "`(x[0] + x[1]) + (x[2] + x[3])`\n",
    "\n",
    "It's also possible for an implementation to use an accumulator variable of higher precision, and only round at the end which produces different results.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv2d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Like torch's conv2d using bias=False and all other keyword arguments left at their default values.\n",
    "\n",
    "    x: shape (batch, in_channels, height, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_height, output_width)\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    B, iC, iH, iW = x.shape\n",
    "    oC, iCW, kH, kW = weights.shape\n",
    "    assert iC == iCW\n",
    "    oH = iH - kH + 1\n",
    "    oW = iW - kW + 1\n",
    "    xs = x.stride()\n",
    "    assert len(xs) == 4\n",
    "    conv_size = (B, iC, oH, oW, kH, kW)\n",
    "    conv_stride = (xs[0], xs[1], xs[2], xs[3], xs[2], xs[3])\n",
    "    strided_x = x.as_strided(size=conv_size, stride=conv_stride)\n",
    "    return t.einsum(\"bcxyij,ocij->boxy\", strided_x, weights)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_conv2d_minimal(conv2d_minimal, t.float64, 1e-10)\n",
    "    w1d2_test.test_conv2d_minimal(conv2d_minimal, t.float32, 1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding a Tensor\n",
    "\n",
    "For `conv` and the following `maxpool` you'll need to implement `pad` helper functions. PyTorch has some very generic padding functions, but to keep things simple and build up gradually, we'll write 1D and 2D functions individually.\n",
    "\n",
    "Tip: use the `new_full` method of the input tensor. This is a clean way to ensure that the output tensor is on the same device as the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad1d(x: t.Tensor, left: int, right: int, pad_value: float) -> t.Tensor:\n",
    "    \"\"\"Return a new tensor with padding applied to the edges.\n",
    "\n",
    "    x: shape (batch, in_channels, width), dtype float32\n",
    "\n",
    "    Return: shape (batch, in_channels, left + right + width)\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    assert left >= 0 and right >= 0\n",
    "    B, C, W = x.shape\n",
    "    out = x.new_full((B, C, left + W + right), pad_value)\n",
    "    out[..., left : left + W] = x\n",
    "    return out\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_pad1d(pad1d)\n",
    "    w1d2_test.test_pad1d_multi_channel(pad1d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad2d(x: t.Tensor, left: int, right: int, top: int, bottom: int, pad_value: float) -> t.Tensor:\n",
    "    \"\"\"Return a new tensor with padding applied to the edges.\n",
    "\n",
    "    x: shape (batch, in_channels, height, width), dtype float32\n",
    "\n",
    "    Return: shape (batch, in_channels, top + height + bottom, left + width + right)\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    assert left >= 0 and right >= 0 and top >= 0 and bottom >= 0\n",
    "    B, C, H, W = x.shape\n",
    "    out = x.new_full((B, C, top + H + bottom, left + W + right), pad_value)\n",
    "    out[..., top : top + H, left : left + W] = x\n",
    "    return out\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_pad2d(pad2d)\n",
    "    w1d2_test.test_pad2d_multi_channel(pad2d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Padding and Stride for `conv1d`\n",
    "\n",
    "Now extend `conv1d` to handle the `stride` and `padding` arguments.\n",
    "\n",
    "`stride` is the number of input positions that the kernel slides at each step.\n",
    "`padding` is the number of zeros concatenated to each side of the input before the convolution.\n",
    "\n",
    "Output shape should be (batch, output_channels, output_length), where output_length can be calculated as follows:\n",
    "$$\n",
    "\\text{output\\_length} = \\left\\lfloor\\frac{\\text{input\\_length} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "Verify for yourself that the forumla above simplifies to the formula we used earlier when padding is 0 and stride is 1.\n",
    "\n",
    "Docs for pytorch's conv1d can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html).\n",
    "\n",
    "<details>\n",
    "<summary>Hint - how to use the stride argument</summary>\n",
    "When you perform as_strided on the input, there are two output dimensions that are derived from x.stride()[-1]. One of them should use this value without modification; the other one will use this value multiplied by the stride argument.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(x, weights, stride: int = 1, padding: int = 0) -> t.Tensor:\n",
    "    \"\"\"Like torch's conv1d using bias=False.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_width)\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    B, iC, iW = x.shape\n",
    "    oC, _, kW = weights.shape\n",
    "    oW = (iW + 2 * padding - kW) // stride + 1\n",
    "\n",
    "    padded_x = pad1d(x, padding, padding, 0.0)\n",
    "    conv_size = (B, iC, oW, kW)\n",
    "    bs, cs, ws = padded_x.stride()  # type: ignore\n",
    "    conv_stride = (bs, cs, ws * stride, ws)\n",
    "    strided_x = t.as_strided(padded_x, size=conv_size, stride=conv_stride)\n",
    "\n",
    "    return t.einsum(\"bcxi,oci->box\", strided_x, weights)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_conv1d(conv1d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Helper functions for pairs\n",
    "\n",
    "A recurring pattern in these 2d functions is allowing the user to specify either an int or a pair of ints for an argument: examples are stride and padding. We've provided some type aliases and a helper function to simplify working with these.\n",
    "\"\"\"\n",
    "IntOrPair = Union[int, Tuple[int, int]]\n",
    "Pair = Tuple[int, int]\n",
    "\n",
    "\n",
    "def force_pair(v: IntOrPair) -> Pair:\n",
    "    \"\"\"Convert v to a pair of int, if it isn't already.\"\"\"\n",
    "    if isinstance(v, tuple):\n",
    "        if len(v) != 2:\n",
    "            raise ValueError(v)\n",
    "        return int(v[0]), int(v[1])  # cast np.int32, torch scalars etc\n",
    "    elif isinstance(v, int):\n",
    "        return (v, v)\n",
    "    raise ValueError(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Padding and Stride for `conv2d`\n",
    "\n",
    "\n",
    "Note the type signature: stride and padding can be either a single number or a tuple for each sliding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv2d(x, weights, stride: IntOrPair = 1, padding: IntOrPair = 0) -> t.Tensor:\n",
    "    \"\"\"Like torch's conv2d using bias=False\n",
    "\n",
    "    x: shape (batch, in_channels, height, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_height, output_width)\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    sH, sW = force_pair(stride)\n",
    "    pH, pW = force_pair(padding)\n",
    "    B, iC, iH, iW = x.shape\n",
    "    oC, _, kH, kW = weights.shape\n",
    "    oH = (iH + 2 * pH - kH) // sH + 1\n",
    "    oW = (iW + 2 * pW - kW) // sW + 1\n",
    "    padded_x = pad2d(x, pW, pW, pH, pH, 0.0)\n",
    "    conv_size = (B, iC, oH, oW, kH, kW)\n",
    "    bs, cs, hs, ws = padded_x.stride()  # type: ignore\n",
    "    conv_stride = (bs, cs, hs * sH, ws * sW, hs, ws)\n",
    "    strided_x = t.as_strided(padded_x, size=conv_size, stride=conv_stride)\n",
    "\n",
    "    return t.einsum(\"bcxyij,ocij->boxy\", strided_x, weights)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_conv2d(conv2d, t.float64, 1e-10)\n",
    "    w1d2_test.test_conv2d(conv2d, t.float32, 1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Max Pooling\n",
    "\n",
    "A \"max pooling\" layer is similar to a convolution in that you have a window sliding over some number of dimensions. The main difference is that there's no kernel: instead of multiplying by the kernel and adding, you just take the maximum.\n",
    "\n",
    "The way multiple channels work is also different. A convolution has some number of input and output channels, and each output channel is a function of all the input channels. There can be any number of output channels. In a pooling layer, the maximum operation is applied independently for each input channel, meaning the number of output channels is necessarily equal to the number of input channels.\n",
    "\n",
    "Implement `MaxPool2d` using `torch.as_strided` and `torch.amax` together. Your version should behave the same as the [PyTorch version](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) but only the indicated arguments need to be supported.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Help! My forward returns the right shape but wrong values!</summary>\n",
    "\n",
    "Try using an input with all positive values. Does it work correctly now?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Help! I'm still confused!</summary>\n",
    "\n",
    "The most common cause of this is a wrong padding value. You used zero for the padding value in your convolution. Here, it should be a value smaller than any valid input value, namely `float('-inf')`. A real implementation would need to care about integer tensors as well, but you can punt on this for today.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def maxpool2d(\n",
    "    x: t.Tensor,\n",
    "    kernel_size: IntOrPair,\n",
    "    stride: Optional[IntOrPair] = None,\n",
    "    padding: IntOrPair = 0,\n",
    ") -> t.Tensor:\n",
    "    \"\"\"Like PyTorch's maxpool2d.\n",
    "\n",
    "    x: shape (batch, channels, height, width)\n",
    "    stride: if None, should be equal to the kernel size\n",
    "\n",
    "    Return: (batch, channels, out_height, out_width)\n",
    "    \"\"\"\n",
    "    \"SOLUTION\"\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    B, iC, iH, iW = x.shape\n",
    "    kH, kW = force_pair(kernel_size)\n",
    "    sH, sW = force_pair(stride)\n",
    "    pH, pW = force_pair(padding)\n",
    "    oH = (iH + 2 * pH - kH) // sH + 1\n",
    "    oW = (iW + 2 * pW - kW) // sW + 1\n",
    "    padded_x = pad2d(x, pW, pW, pH, pH, -float(\"inf\"))\n",
    "    conv_size = (B, iC, oH, oW, kH, kW)\n",
    "    bs, cs, hs, ws = padded_x.stride()  # type: ignore\n",
    "    conv_stride = (bs, cs, hs * sH, ws * sW, hs, ws)\n",
    "    strided_x = t.as_strided(padded_x, size=conv_size, stride=conv_stride)\n",
    "    return strided_x.amax((-2, -1))\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_maxpool2d(maxpool2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Module version\n",
    "\n",
    "You've written the functional form of max pooling - next we'll do the `nn.Module` version. A brief primer on `nn.Module` can be found [here](https://www.notion.so/Subclassing-nn-Module-878cd4a1972a4aca822830e49bbaabf4).\n",
    "\n",
    "Since this module has no learnable parameters, there's only a few steps to do:\n",
    "\n",
    "### Constructor\n",
    "\n",
    "Remember to call `super().__init__()` in all your `Module` subclasses.\n",
    "\n",
    "Store the input arguments and handle the case where stride is `None`.\n",
    "\n",
    "It would be possible to validate (ensure `kernel_size` is positive) and standardize all the arguments here (converting ints to tuples, etc) and throw an exception if anything is wrong, which would save users some time instead of waiting until `forward()` throws an exception. We will follow PyTorch in not bothering to do this.\n",
    "\n",
    "### extra_repr\n",
    "\n",
    "One consequence of subclassing `nn.Module` is that this method is called when you `repr()` an instance of this class, such as printing it in a REPL or notebook. It should provide a human-readable string representation of the attributes of the module that we might want to know about. In this case, these are provided as arguments to the constructor: `kernel_size`, `stride`, and `padding`. You may want to delegate this to a helper function, since you'll be writing this method for every `Module` you implement today that has arguments. Hint: you can use `getattr(object, name)` to programmatically look up `object.name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if \"SOLUTION\":\n",
    "\n",
    "    def extra_repr(module, arg_names: List[str], kwarg_names: List[str]) -> str:\n",
    "        reprs = [repr(getattr(module, arg_name)) for arg_name in arg_names] + [\n",
    "            f\"{k}={getattr(module, k)}\" for k in kwarg_names\n",
    "        ]\n",
    "        return \", \".join(reprs)\n",
    "\n",
    "\n",
    "class MaxPool2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: IntOrPair,\n",
    "        stride: Optional[IntOrPair] = None,\n",
    "        padding: IntOrPair = 1,\n",
    "    ):\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = kernel_size if stride is None else stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Call the functional version of maxpool2d.\"\"\"\n",
    "        \"SOLUTION\"\n",
    "        return maxpool2d(\n",
    "            x,\n",
    "            self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"Add additional information to the string representation of this class.\"\"\"\n",
    "        \"SOLUTION\"\n",
    "        return extra_repr(self, [], [\"kernel_size\", \"stride\", \"padding\"])\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_maxpool2d_module(MaxPool2d)\n",
    "    m = MaxPool2d(3, stride=2, padding=1)\n",
    "    print(f\"Manually verify that this is an informative repr: {m}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Module\n",
    "\n",
    "Implement your own `Linear` module. This is the first `Module` that has learnable weights and biases. What type should these variables be?\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Solution</summary>\n",
    "\n",
    "It has to be a `torch.Tensor` wrapped in a `nn.Parameter` in order for `nn.Module` to recognize it. If you forget to do this, `module.parameters()` won't include your `Parameter`, which prevents an optimizer from being able to modify it during training.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Initialization\n",
    "\n",
    "For any layer, initialization is very important for the stability of training: with a bad initialization, your model will take much longer to converge or may completely fail to learn anything. The default PyTorch behavior isn't necessarily optimal and you can often improve performance by using something more custom, but we'll follow it for today because it's simple and works decently well.\n",
    "\n",
    "Each float in the weight and bias tensors are drawn independently from the uniform distribution on the interval:\n",
    "\n",
    "$$ -\\frac{1}{\\sqrt{fan_{in}}}, \\frac{1}{\\sqrt{fan_{in}}} $$\n",
    "\n",
    "Where $fan_{in}$ is the number of input features.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Help! I get the error 'RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.'</summary>\n",
    "\n",
    "You should initialize the weight `Tensor` before you wrap it in a `Parameter`, because the `Parameter` sets `requires_grad=True` on its argument and this isn't desirable. We don't actually want to backprop through the initialization.\n",
    "\n",
    "</details>\n",
    "\n",
    "For the `forward` method, remember that \"...\" can be used to represent unnamed dimensions in an einsum. (Docs [here](https://pytorch.org/docs/stable/generated/torch.einsum.html), search for \"Ellipsis\".)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
    "        \"\"\"A simple linear (technically, affine) transformation.\n",
    "\n",
    "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
    "        If `bias` is False, set `self.bias` to None.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        super().__init__()\n",
    "        bound = in_features**-0.5\n",
    "        self.weight = nn.parameter.Parameter(t.empty(out_features, in_features).uniform_(-bound, bound))\n",
    "        if bias:\n",
    "            self.bias = nn.parameter.Parameter(t.empty(out_features).uniform_(-bound, bound))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (*, in_features)\n",
    "        Return: shape (*, out_features)\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        x = t.einsum(\"...j,kj->...k\", x, self.weight)\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"SOLUTION\"\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_linear_forward(Linear)\n",
    "    w1d2_test.test_linear_parameters(Linear)\n",
    "    w1d2_test.test_linear_no_bias(Linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `Conv2d` - nn.Module version\n",
    "\n",
    "The initialization of the Conv2d is also very important. PyTorch does the same uniform distribution, considering `in_features` to be the number of inputs contributing to each output value: `in_channels * product(kernel_size)`.\n",
    "\n",
    "Remember to use `force_pair` before using `kernel_size`, `stride`, and `padding`, which could come as ints or pairs to the constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv2d(t.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: IntOrPair,\n",
    "        stride: IntOrPair = 1,\n",
    "        padding: IntOrPair = 0,\n",
    "    ):\n",
    "        \"\"\"Same as torch.nn.Conv2d with bias=False.\n",
    "\n",
    "        Name your weight field `self.weight` for compatibility with the PyTorch version.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = force_pair(kernel_size)\n",
    "        self.stride = force_pair(stride)\n",
    "        self.padding = force_pair(padding)\n",
    "\n",
    "        in_features = in_channels * self.kernel_size[0] * self.kernel_size[1]\n",
    "        bound = in_features**-0.5\n",
    "        self.weight = nn.parameter.Parameter(\n",
    "            t.empty((out_channels, in_channels, *self.kernel_size)).uniform_(-bound, bound)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Apply the functional conv2d you wrote earlier.\"\"\"\n",
    "        \"SOLUTION\"\n",
    "        return conv2d(x, self.weight, stride=self.stride, padding=self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"\"\"\"\n",
    "        \"SOLUTION\"\n",
    "        return extra_repr(self, [\"in_channels\", \"out_channels\"], [\"kernel_size\", \"stride\"])\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    print(f\"Manually verify that this is a useful looking repr: {Conv2d(1, 2, (3, 4), padding=5)}\")\n",
    "    w1d2_test.test_conv2d_module(Conv2d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Batch Normalization\n",
    "\n",
    "Previous modules you've implemented have either held no tensors like MaxPool2d, or held learnable parameters like Conv2d. The `BatchNorm2d` has a third category: **buffers**.\n",
    "\n",
    "Unlike `nn.Parameter`, a buffer is not its own type and does not wrap a `Tensor`. A buffer is just a regular `Tensor` on which you've called [self.register_buffer](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer) from inside a `nn.Module`.\n",
    "\n",
    "The reason we have a different name for this is to describe how it is treated by various machinery within PyTorch.\n",
    "\n",
    "- It is normally included in the output of `module.state_dict()`, meaning that `torch.save` and `torch.load` will serialize and deserialize it.\n",
    "- It is moved between devices when you call `model.to(device)`.\n",
    "- It IS NOT included in `module.parameters`, so optimizers won't see or modify it. Instead, your module will modify it as appropriate within `forward`.\n",
    "\n",
    "### Train and Eval Modes\n",
    "\n",
    "This is your first implementation that needs to care about the value of `self.training`, which is set to True by default, and can be set to False by `self.eval()` or to True by `self.train()`.\n",
    "\n",
    "In training mode, you should use the mean and variance of the batch you're on, but you should also update a stored `running_mean` and `running_var` on each call to `forward` using the \"momentum\" argument as described in the PyTorch docs. Your `running_mean` shuld be intialized as all zeros; your `running_var` should be initialized as all ones.\n",
    "\n",
    "In eval mode, you should use the running mean and variance that you stored before (not the mean and variance from the current batch).\n",
    "\n",
    "Implement `BatchNorm2d` according to the [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html). Call your learnable parameters `weight` and `bias` for consistency with PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BatchNorm2d(nn.Module):\n",
    "\n",
    "    running_mean: t.Tensor\n",
    "    \"\"\"running_mean: shape (num_features,)\"\"\"\n",
    "    running_var: t.Tensor\n",
    "    \"\"\"running_var: shape (num_features,)\"\"\"\n",
    "    num_batches_tracked: t.Tensor\n",
    "    \"\"\"num_batches_tracked: shape ()\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps=1e-5,\n",
    "        momentum=0.1,\n",
    "    ):\n",
    "        \"\"\"Like nn.BatchNorm2d with track_running_stats=True and affine=True.\n",
    "\n",
    "        Name the learnable affine parameters `weight` and `bias` in that order.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.weight = nn.parameter.Parameter(t.ones(num_features))\n",
    "        self.bias = nn.parameter.Parameter(t.zeros(num_features))\n",
    "        self.register_buffer(\"running_mean\", t.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", t.ones(num_features))\n",
    "        self.register_buffer(\"num_batches_tracked\", t.tensor(0))\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Normalize each channel.\n",
    "\n",
    "        Compute the variance using `torch.var(x, unbiased=False)`\n",
    "\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels, height, width)\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        dims = (0, 2, 3)\n",
    "        if self.training:\n",
    "            mean = x.mean(dims)\n",
    "            var = x.var(dims, unbiased=False)\n",
    "            m = self.momentum\n",
    "            # Equivalent, maybe slower than inplace but I didn't benchmark\n",
    "            # You can also assign to .data but typechecker doesn't like it and maybe slower\n",
    "            # self.running_mean = self.running_mean * (1 - m) + m * mean\n",
    "            self.running_mean.mul_(1 - m).add_(m * mean)\n",
    "            self.running_var.mul_(1 - m).add_(m * var)\n",
    "            self.num_batches_tracked.data += 1\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        rs = lambda u: u.reshape(1, -1, 1, 1)\n",
    "        return rs(self.weight) * (x - rs(mean)) / t.sqrt(rs(var) + self.eps) + rs(self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"SOLUTION\"\n",
    "        return extra_repr(self, [\"num_features\"], [\"eps\", \"momentum\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_batchnorm2d_module(BatchNorm2d)\n",
    "    w1d2_test.test_batchnorm2d_forward(BatchNorm2d)\n",
    "    w1d2_test.test_batchnorm2d_running_mean(BatchNorm2d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ReLU Module\n",
    "\n",
    "Write the module version of ReLU using `torch.maximum`. Its constructor has no arguments, so it doesn't need an `extra_repr`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReLU(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"SOLUTION\"\n",
    "        return t.maximum(x, t.tensor(0.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sequential\n",
    "\n",
    "[torch.nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) has a number of features, but we'll just implement the basics that we need for today.\n",
    "\n",
    "Note: the base class's `repr` already recursively prints out the submodules, so you don't need to write anything in `extra_repr`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sequential(nn.Module):\n",
    "    def __init__(self, *modules: nn.Module):\n",
    "        \"\"\"\n",
    "        Call `self.add_module` on each provided module, giving each one a unique (within this Sequential) name.\n",
    "        Internally, this adds them to the dictionary `self._modules` in the base class, which means they'll be included in self.parameters() as desired.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        for i, mod in enumerate(modules):\n",
    "            self.add_module(str(i), mod)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Chain each module together, with the output from one feeding into the next one.\"\"\"\n",
    "        \"SOLUTION\"\n",
    "        for mod in self._modules.values():\n",
    "            assert mod is not None, \"self._modules can contain None values in general, but Sequentials shouldn't\"\n",
    "            x = mod(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_sequential(Sequential)\n",
    "    w1d2_test.test_sequential_forward(Sequential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Flatten\n",
    "\n",
    "Implement your own [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html).\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Spoiler - Which function do I use to perform the flattening?</summary>\n",
    "\n",
    "`torch.reshape` is the best choice since it has the desired behavior: returning a view when possible, otherwise a copy. You could also use `einops.rearrange` but you'd have to construct the rearrangement pattern as a string.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self, start_dim: int = 1, end_dim: int = -1) -> None:\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Flatten out dimensions from start_dim to end_dim, inclusive of both.\n",
    "\n",
    "        Return a view if possible, otherwise a copy.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        end_dim = self.end_dim + (input.ndim if self.end_dim < 0 else 0)\n",
    "        out_shape = (\n",
    "            [input.shape[i] for i in range(0, self.start_dim)]\n",
    "            + [-1]\n",
    "            + [input.shape[i] for i in range(end_dim + 1, input.ndim)]\n",
    "        )\n",
    "        return input.reshape(out_shape)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"SOLUTION\"\n",
    "        return extra_repr(self, [], [\"start_dim\", \"end_dim\"])\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    w1d2_test.test_flatten(Flatten)\n",
    "    w1d2_test.test_flatten_is_view(Flatten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ResNet Average Pooling\n",
    "\n",
    "Let's end our collection of `Module`s with an easy one :)\n",
    "\n",
    "The ResNet has a Linear layer with 1000 outputs at the end in order to produce classification logits for each of the 1000 classes. Any Linear needs to have a constant number of input features, but the ResNet is supposed to be compatible with arbitrary height and width, so we can't just do a pooling operation with a fixed kernel size and stride.\n",
    "\n",
    "Luckily, the simplest possible solution works decently: take the mean over the spatial dimensions. Intuitively, each position has an equal \"vote\" for what objects it can \"see\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AveragePool(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels)\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        return x.mean((-2, -1))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assembling ResNet\n",
    "\n",
    "Now we have all the building blocks we need to start assembling your own ResNet! The following diagram describes the architecture of ResNet34 - the other versions are broadly similar. Unless otherwise noted, convolutions have a kernel_size of 3x3 and a stride of 1. None of the convolutions have biases. You'll be able to infer the number of input channels by the output of the previous layer, and infer the necessary padding from the input and output shapes.\n",
    "\n",
    "Exercise: there would be no advantage to enabling biases on the convolutional layers. Why?\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Solution - Why No Biases?</summary>\n",
    "\n",
    "Every convolution layer in this network is followed by a batch normalization layer. The first operation in the batch normalization layer is to subtract the mean of each output channel. But a convolutional bias just adds some scalar `b` to each output channel, increasing the mean by `b`. This means that for any `b` added, the batch normalization will subtract `b` to exactly negate the bias term.\n",
    "\n",
    "</details>\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "subgraph \" \"\n",
    "    subgraph ResNet34\n",
    "        Input[Input<br>] --> InConv[7x7 Conv<br>64 channels, stride 2] --> InBN[BatchNorm] --> InReLU[ReLU] --> InPool[3x3 MaxPool<br>Stride 2] --> BlockGroups[BlockGroups<br>0, 1, ..., N] --> AveragePool --> Flatten --> Linear[Linear<br/>1000 outputs] --> Out\n",
    "    end\n",
    "\n",
    "    subgraph BlockGroup\n",
    "        BGIn[Input] --> DRB[ResidualBlock<br>WITH optional part] --> RB0[ResidualBlocks<br>0, 1, ..., N<br>WITHOUT optional part] --> BGOut[Out]\n",
    "    end\n",
    "\n",
    "    subgraph ResidualBlock\n",
    "        BIn[Input] --> BConv[Strided Conv] --> BBN1[BatchNorm] --> ReLU --> BConv2[Conv] --> BBN2[BatchNorm] --> Add --> ReLu2[ReLU] --> RBOut[Out]\n",
    "                BIn --> DBConvD[OPTIONAL<br>1x1 Strided Conv] --> DBDBatchNorm[OPTIONAL<br>BatchNorm] --> Add\n",
    "    end\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block\n",
    "\n",
    "Implement `ResidualBlock` by referring to the diagram. The number of channels changes from `in_feats` to `out_feats` at the first convolution in each branch.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>I'm confused about where to apply the stride argument!</summary>\n",
    "\n",
    "The stride only applies to first convolution in each branch. If you apply it to the second convolution in the left branch, the add won't work because you'll have shrunk the left branch twice.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mResidualBlock\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_feats: \u001b[39mint\u001b[39m, out_feats: \u001b[39mint\u001b[39m, first_stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[39m\"\"\"A single residual block with optional downsampling.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[39m        For compatibility with the pretrained model, declare the left side branch first using a `Sequential`.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[39m        If first_stride is > 1, this means the optional (conv + bn) should be present on the right branch. Declare it second using another `Sequential`.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"A single residual block with optional downsampling.\n",
    "\n",
    "        For compatibility with the pretrained model, declare the left side branch first using a `Sequential`.\n",
    "\n",
    "        If first_stride is > 1, this means the optional (conv + bn) should be present on the right branch. Declare it second using another `Sequential`.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        self.net = Sequential(\n",
    "            Conv2d(in_feats, out_feats, kernel_size=3, stride=first_stride, padding=1),\n",
    "            BatchNorm2d(out_feats),\n",
    "            ReLU(),\n",
    "            Conv2d(out_feats, out_feats, kernel_size=3, padding=1),\n",
    "            BatchNorm2d(out_feats),\n",
    "        )\n",
    "        self.downsample = (\n",
    "            Sequential(\n",
    "                Conv2d(in_feats, out_feats, kernel_size=1, stride=first_stride),\n",
    "                BatchNorm2d(out_feats),\n",
    "            )\n",
    "            if first_stride != 1\n",
    "            else None\n",
    "        )\n",
    "        self.out_relu = ReLU()\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / stride, width / stride)\n",
    "\n",
    "        If no downsampling block is present, the addition should just add the left branch's output to the input.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        y_out = self.net(x)\n",
    "        x_out = x if self.downsample is None else self.downsample(x)\n",
    "        out = self.out_relu(x_out + y_out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block\n",
    "\n",
    "Implement `ResidualBlock` by referring to the diagram. The number of channels changes from `in_feats` to `out_feats` at the first convolution in each branch.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>I'm confused about where to apply the stride argument!</summary>\n",
    "\n",
    "The stride only applies to first convolution in each branch. If you apply it to the second convolution in the left branch, the add won't work because you'll have shrunk the left branch twice.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"A single residual block with optional downsampling.\n",
    "\n",
    "        For compatibility with the pretrained model, declare the left side branch first using a `Sequential`.\n",
    "\n",
    "        If first_stride is > 1, this means the optional (conv + bn) should be present on the right branch. Declare it second using another `Sequential`.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        self.net = Sequential(\n",
    "            Conv2d(in_feats, out_feats, kernel_size=3, stride=first_stride, padding=1),\n",
    "            BatchNorm2d(out_feats),\n",
    "            ReLU(),\n",
    "            Conv2d(out_feats, out_feats, kernel_size=3, padding=1),\n",
    "            BatchNorm2d(out_feats),\n",
    "        )\n",
    "        self.downsample = (\n",
    "            Sequential(\n",
    "                Conv2d(in_feats, out_feats, kernel_size=1, stride=first_stride),\n",
    "                BatchNorm2d(out_feats),\n",
    "            )\n",
    "            if first_stride != 1\n",
    "            else None\n",
    "        )\n",
    "        self.out_relu = ReLU()\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / stride, width / stride)\n",
    "\n",
    "        If no downsampling block is present, the addition should just add the left branch's output to the input.\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        y_out = self.net(x)\n",
    "        x_out = x if self.downsample is None else self.downsample(x)\n",
    "        out = self.out_relu(x_out + y_out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### BlockGroup\n",
    "\n",
    "Implement BlockGroup according to the diagram. The number of channels changes from `in_feats` to `out_feats` at the first convolution in the BlockGroup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGroup(nn.Module):\n",
    "    def __init__(self, n_blocks: int, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"An n_blocks-long sequence of ResidualBlock where only the first block uses the provided stride.\"\"\"\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        self.blocks = Sequential(\n",
    "            ResidualBlock(in_feats, out_feats, first_stride),\n",
    "            *(ResidualBlock(out_feats, out_feats) for _ in range(n_blocks - 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Compute the forward pass.\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        \"\"\"\n",
    "        return self.blocks(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ResNet34\n",
    "\n",
    "Last step! Assemble `ResNet34` using the diagram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks_per_group=[3, 4, 6, 3],\n",
    "        out_features_per_group=[64, 128, 256, 512],\n",
    "        strides_per_group=[1, 2, 2, 2],\n",
    "        n_classes=1000,\n",
    "    ):\n",
    "        \"SOLUTION\"\n",
    "        super().__init__()\n",
    "        in_feats0 = 64\n",
    "\n",
    "        self.in_layers = Sequential(\n",
    "            Conv2d(3, in_feats0, kernel_size=7, stride=2, padding=3),\n",
    "            BatchNorm2d(in_feats0),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        all_in_feats = [in_feats0] + out_features_per_group[:-1]\n",
    "        self.residual_layers = Sequential(\n",
    "            *(\n",
    "                BlockGroup(*args)\n",
    "                for args in zip(\n",
    "                    n_blocks_per_group,\n",
    "                    all_in_feats,\n",
    "                    out_features_per_group,\n",
    "                    strides_per_group,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.out_layers = Sequential(\n",
    "            AveragePool(),\n",
    "            Flatten(),\n",
    "            Linear(512, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, channels, height, width)\n",
    "\n",
    "        Return: shape (batch, n_classes)\n",
    "        \"\"\"\n",
    "        \"SOLUTION\"\n",
    "        x = self.in_layers(x)\n",
    "        x = self.residual_layers(x)\n",
    "        x = self.out_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Copying Pretrained Weights\n",
    "\n",
    "Before we go training our ResNet from scratch, let's make sure everything is correct by copying over the weights from the `torchvision` version to yours. Call `state_dict()` on the torchvision model to retrieve its state, and then convert that into a dictionary that you can feed to your model's `load_state_dict`.\n",
    "\n",
    "You can do this however you want, but way I prefer is to try and build a 1-1 correspondence between the order of the 218 parameters and buffers in the pretrained model and the 218 in yours. Then you can just pair the names from your model with the data from their model. If you've followed the instructions throughout for what order to initialize things in, then this should \"just work\".\n",
    "\n",
    "This will be tedious and if you really hate this, it might be a sign that ML engineering is not for you because this is a realistic representation of an actual task. You'll be doing a very similar process for BERT and GPT-2 models later :)\n",
    "\n",
    "Tip: `load_state_dict` expects an `OrderedDict`, but since Python 3.7, the builtin `dict` is guaranteed to maintain items in the order they're inserted, so you can safely use a regular `dict`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAIN and not IS_CI:\n",
    "    if \"SOLUTION\":\n",
    "        their_model = models.resnet34(pretrained=True)\n",
    "        your_model = ResNet34()\n",
    "\n",
    "        their_state = their_model.state_dict().items()\n",
    "        your_state = your_model.state_dict().items()\n",
    "        assert len(their_state) == len(your_state), \"Differing number of saved state tensors!\"\n",
    "\n",
    "        \"\"\"I like to see all the names and shapes so I know where they don't line up.\"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.DataFrame.from_records(\n",
    "            [(tk, tuple(tv.shape), mk, tuple(mv.shape)) for ((tk, tv), (mk, mv)) in zip(their_state, your_state)],\n",
    "            columns=[\"their name\", \"their shape\", \"your name\", \"your shape\"],\n",
    "        )\n",
    "        with pd.option_context(\"display.max_rows\", None):  # type: ignore\n",
    "            display(df)\n",
    "\n",
    "        assert all(x[1].shape == y[1].shape for x, y in zip(their_state, your_state)), \"Shapes don't match!\"\n",
    "        d = OrderedDict((yours[0], theirs[1]) for yours, theirs in zip(your_state, their_state))\n",
    "        your_model.load_state_dict(d, strict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Your Model\n",
    "\n",
    "Now the moment of truth: call the `predict` function that you wrote way back on an instance of your own `ResNet34` class. If you've done everything correctly, your version should give the same classifications, and the percentages should match at least to a couple decimal places.\n",
    "\n",
    "If it does, congratulations, you've now run an entire ResNet, using barely any code from `torch.nn`! The only things we used were `nn.Module` and `nn.Parameter`, and you'll reimplement those in W1D3.\n",
    "\n",
    "If it doesn't, congratulations, you get to practice model debugging! Don't be afraid to call a TA here if you get stuck.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Help! My model is predicting roughly the same percentage for every category!</summary>\n",
    "\n",
    "This can indicate that your model weights are randomly initialized, meaning the weight loading process didn't actually take. Or, you reinitialized your model by accident after loading the weights.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Help! My model is outputting `nan`!</summary>\n",
    "\n",
    "To debug this, find the first place where `nan` appears. One way to do this is with forward hooks. An example of using hooks is given below.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_hook(module: nn.Module, inputs, output):\n",
    "    \"\"\"Example of a hook function that can be registered to a module.\"\"\"\n",
    "    x = inputs[0]\n",
    "    if t.isnan(x).any():\n",
    "        raise ValueError(module, x)\n",
    "    out = output[0]\n",
    "    if t.isnan(out).any():\n",
    "        raise ValueError(module, x)\n",
    "\n",
    "\n",
    "def add_hook(module: nn.Module) -> None:\n",
    "    \"\"\"Remove any existing hooks and register our hook.\n",
    "\n",
    "    Use model.apply(add_hook) to recursively apply the hook to model and all submodules.\n",
    "    \"\"\"\n",
    "    utils.remove_hooks(module)\n",
    "    module.register_forward_hook(check_nan_hook)\n",
    "\n",
    "\n",
    "if MAIN and not IS_CI:\n",
    "    your_model.apply(add_hook)\n",
    "    your_model_predictions = predict(your_model, images)\n",
    "    w1d2_test.test_same_predictions(your_model_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training ResNet on CIFAR10\n",
    "\n",
    "Next, we'll train our ResNet from scratch, on the GPU.\n",
    "\n",
    "ImageNet is going to take too long, so we'll use a much smaller dataset called CIFAR10, which has only 10 classes and much smaller images.\n",
    "\n",
    "### Preparing the CIFAR10 Data\n",
    "\n",
    "The data preparation is the same as before, so we've provided it to save time. Following good practice, we'll verify that preprocessed data is roughly normalized to mean 0 and std 1, and looks reasonable visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cifar_classes = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_cifar10():\n",
    "    \"\"\"Download (if necessary) and return the CIFAR10 dataset.\"\"\"\n",
    "\n",
    "    \"\"\"The following is a workaround for this bug: https://github.com/pytorch/vision/issues/5039\"\"\"\n",
    "    if sys.platform == \"win32\":\n",
    "        import ssl\n",
    "\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    \"\"\"Magic constants taken from: https://docs.ffcv.io/ffcv_examples/cifar10.html\"\"\"\n",
    "    mean = t.tensor([125.307, 122.961, 113.8575]) / 255\n",
    "    std = t.tensor([51.5865, 50.847, 51.255]) / 255\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cifar_train = torchvision.datasets.CIFAR10(\"w1d2_cifar10_train\", transform=transform, download=True, train=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(\n",
    "        \"w1d2_cifar10_train\",\n",
    "        transform=transform,\n",
    "        download=True,\n",
    "        train=False,\n",
    "    )\n",
    "    return cifar_train, cifar_test\n",
    "\n",
    "\n",
    "if MAIN and not IS_CI:\n",
    "    cifar_train, cifar_test = get_cifar10()\n",
    "    trainloader = DataLoader(cifar_train, batch_size=512, shuffle=True, pin_memory=True)\n",
    "    testloader = DataLoader(cifar_test, batch_size=512, pin_memory=True)\n",
    "\n",
    "# %%\n",
    "if MAIN and not IS_CI:\n",
    "    batch = next(iter(trainloader))\n",
    "    print(\"Mean value of each channel: \", batch[0].mean((0, 2, 3)))\n",
    "    print(\"Std value of each channel: \", batch[0].std((0, 2, 3)))\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=5, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):  # type: ignore\n",
    "        ax.imshow(rearrange(batch[0][i], \"c h w -> h w c\"))\n",
    "        ax.set(xlabel=cifar_classes[batch[1][i].item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train your ResNet\n",
    "\n",
    "We've provided a basic training loop which is far from optimal, but will serve for today. On my GTX 1080Ti, it took 30 seconds per epoch. On a Tesla V100, it took around 20 seconds per epoch. If you don't have access to a GPU, it's going to be pretty slow and you should reduce the number of epochs from the default of 8.\n",
    "\n",
    "You may encounter some issues running on GPU. The most common issue is if you manually created any tensors, they could be on the wrong device. The PyTorch docs has a section on [creation ops](https://pytorch.org/docs/stable/torch.html#tensor-creation-ops) which has useful functions for dealing with this cleanly including `empty_like` and `zeros_like`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILENAME = \"./w1d2_resnet34_cifar10.pt\"\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def train(trainloader: DataLoader, epochs: int) -> ResNet34:\n",
    "    model = ResNet34(n_classes=10).to(device).train()\n",
    "    optimizer = t.optim.Adam(model.parameters())\n",
    "    loss_fn = t.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(tqdm(trainloader)):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch}, train loss is {loss}\")\n",
    "        print(f\"Saving model to: {os.path.abspath(MODEL_FILENAME)}\")\n",
    "        t.save(model, MODEL_FILENAME)\n",
    "    return model\n",
    "\n",
    "\n",
    "if MAIN and not IS_CI:\n",
    "    if os.path.exists(MODEL_FILENAME):\n",
    "        print(\"Loading model from disk: \", MODEL_FILENAME)\n",
    "        model = t.load(MODEL_FILENAME)\n",
    "    else:\n",
    "        print(\"Training model from scratch\")\n",
    "        model = train(trainloader, epochs=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test Your ResNet\n",
    "\n",
    "After one epoch, my ResNet achieved 54% accuracy (random guessing would be 10%), train loss of 1.2 and test loss of 1.3.\n",
    "After eight epochs, it achieved 68% accuracy, train loss of 0.4 and test loss of 1.1. This generalization gap means the model is overfitting. By the end of the course you'll be able to do better than this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MAIN and not IS_CI:\n",
    "    model.eval()\n",
    "    model.apply(add_hook)\n",
    "    loss_fn = t.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    with t.inference_mode():\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        loss_total = 0.0\n",
    "        for i, (x, y) in enumerate(tqdm(testloader)):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            with t.autocast(device):\n",
    "                y_hat = model(x)\n",
    "                loss_total += loss_fn(y_hat, y).item()\n",
    "            n_correct += (y_hat.argmax(dim=-1) == y).sum().item()\n",
    "            n_total += len(x)\n",
    "    print(f\"Test accuracy: {n_correct} / {n_total} = {100 * n_correct / n_total:.2f}%\")\n",
    "    print(f\"Test loss: {loss_total / n_total}\")\n",
    "    if \"SKIP\":\n",
    "        assert n_correct / n_total >= 0.60, \"potential regression on test set, or really unlucky\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bonus\n",
    "\n",
    "Congratulations on completing the day's main content! All of the bonus exercises going forward are completely optional, can be done in any order, and nothing later in the course will depend on you having completed them. If you feel uncertain or confused about any of the day's material, it's probably a better idea to just go back and solidify your understanding before tackling these.\n",
    "\n",
    "### Fused BatchNorm and Conv2d\n",
    "\n",
    "After a model is trained, it's possible to merge adjacent BatchNorm and Conv2d layers into one operation for a speedup. Try doing this and see how much improvement you get. If you need a hint, [this blog](https://nenadmarkus.com/p/fusing-batchnorm-and-conv/) has the equations followed by solution code.\n",
    "\n",
    "### Deeper Look at Initialization\n",
    "\n",
    "We did a pretty basic initialization today. Read up on Xavier and Kaiming initializations, implement one or both, and try some experiments to see if they actually work better on CIFAR10 than the default PyTorch behavior.\n",
    "\n",
    "### Residual Block Identity Initialization\n",
    "\n",
    "The ResNet from `torchvision` has a `zero_init_residual` argument. Investigate the source code in `torchvision/models/resnet.py` and see what this does, then replicate it.\n",
    "\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "One way to reduce overfitting is ensure that the network doesn't see the exact same image more than once. Play with some of the transforms in the torchvision library like random crops and rotations and see if you can reduce the generalization gap.\n",
    "\n",
    "### ReLU Benchmarking\n",
    "\n",
    "Predict which of the ReLU functions is fastest on massive tensors, then benchmark them on CPU and GPU. Why do you think some are faster than others?\n",
    "\n",
    "### Negative Strides\n",
    "\n",
    "In NumPy, an `ndarray` can have a negative stride which means iterating backward through the underlying storage. For example: `np.flip(np.arange(10))`. What happens when you do the same thing in PyTorch?\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Solution - Negative Strides In PyTorch</summary>\n",
    "\n",
    "At least in PyTorch 1.11, `torch.flip` makes a copy instead of a view, and negative strides in `as_strided` throw an error. If you are trying to port code that uses `np.flip` to PyTorch, you will probably have to rewrite those calls otherwise it'll be unexpectedly expensive.\n",
    "\n",
    "Supporting negative strides in at least some places is an [open issue](https://github.com/pytorch/pytorch/issues/16424) and if the idea of contributing to PyTorch sounds fun to you, this could be a big adventure.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Adversarial Examples\n",
    "\n",
    "Adversarial examples in this context are images that to a human clearly and obviously belong to one class, but the classifier gives a confidently wrong prediction. Either find existing adversarial examples on the Internet and test them on the various sizes of ResNet, or try to create your own adversarial examples that fool your ResNet. You may find the paper [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/pdf/1905.02175.pdf) to be thought-provoking.\n",
    "\n",
    "### Build Your Own Dtype Support\n",
    "\n",
    "PyTorch has both integer and floating point datatypes. Go through your modules from today and see if your code does the same thing as PyTorch when different datatypes are involved, and fix your code to behave appropriately.\n",
    "\n",
    "### Benchmarking Inference Mode\n",
    "\n",
    "Research what exactly inference mode does, and see if you can detect a difference in speed or memory consumption from using or not using it.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
